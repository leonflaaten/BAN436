{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09-Web scraping\n",
    "\n",
    "This notebook gives an introduction to web scraping in Python using `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web sites often contain large amounts of data.\n",
    "\n",
    "Many web sites have developed *APIs* in order to grant people access to their data. To use an API, we make a request to their web server and if the request is approved, the API returns the requested data. \n",
    "\n",
    "All APIs are different, and in order to know how to use a web site's API, we must read their API documentation (or find an online tutorial!). See e.g. this [tutorial](https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a) for the Twitter API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a lot of data online is not available through an API. In which case, if we want to extract the data, we must do so through *web scraping*. In web scraping, we write programs that extracts information directly from the web site. \n",
    "\n",
    "Unfortunately, there is no ONE way of doing web scraping. What type of information we can extract from a web site and how to extract it varies from web site to web site. In fact many web sites do not want people to scrape their content, and therefore makes it difficult (in some cases, it might even be illegal to scrape the content from their web sites). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at two different ways that we can use `pandas` to scrape content off the web:\n",
    "- Import data from URLs\n",
    "- Read HTML tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from URLs\n",
    "\n",
    "We have seen how to use `read_csv` to import CSV files. However, notice that `read_csv` can also import CSV files directly from an URL (see the [function documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: titanic data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the `titanic` data throughout this course. This is a common data set to work on when learning how to do data science in Python and R. I collected the data set from [this](https://github.com/datasciencedojo/datasets/blob/master/titanic.csv) user on github.\n",
    "\n",
    "Instead of downloading the data file to our computer and then import it using `pandas`, we can import the file directly into our Python program by using the URL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us store the URL in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass the URL directly to `read_csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `titanic` data is a static data set (i.e. it is not likely to change over time). That means that we only need to download it once to our computer. The benefit of importing the data directly from the URL, as opposed to downloading it to our computer first, is therefore small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when the data set is dynamic (i.e. more information is being added over time), there can be large gains from importing the data through the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: covid deaths**\n",
    "\n",
    "The Center for Systems Science and Engineering at Johns Hopkins University has an online repository where they publish data related to covid. The file [time_series_covid19_confirmed_global.csv](https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv) contains global time series on covid deaths, and it is being updated on a daily basis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a program that imports the data, extracts the time series for a specific country and plots the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define url\n",
    "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "\n",
    "# import file from url\n",
    "df_full = pd.read_csv(url)\n",
    "\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract the time series for Norway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df_full.drop(['Province/State', 'Lat', 'Long'], axis = 1, inplace = True)\n",
    "\n",
    "# extract country\n",
    "country = 'Norway'\n",
    "df_subset = df_full[df_full['Country/Region'] == country].copy()\n",
    "\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is *wide* data. We use `melt` to convert the data to a *long* format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt df\n",
    "df_subset = df_subset.melt(id_vars = ['Country/Region'], var_name = 'date', value_name = 'deaths')\n",
    "\n",
    "# convert to datetime\n",
    "df_subset['date'] = pd.to_datetime(df_subset['date'], format = '%m/%d/%y')\n",
    "\n",
    "# rename\n",
    "df_subset.columns = ['country', 'date', 'total']\n",
    "\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the data to plot the cumulative sum of covid deaths in Norway over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 3))\n",
    "\n",
    "ax.plot(df_subset['date'],\n",
    "        df_subset['total'])\n",
    "\n",
    "# set xrange\n",
    "ax.set_xlim(df_subset['date'].min(), df_subset['date'].max())\n",
    "\n",
    "# set title\n",
    "ax.set_title(country + ' (total covid deaths)')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `deaths` column contains cumulative sum of the deaths over time. If we instead want the number of daily deaths, we can use `diff` to calculate the difference in the number of deaths from the day before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['new'] = df_subset['total'].diff()\n",
    "\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 3))\n",
    "\n",
    "ax.plot(df_subset['date'],\n",
    "        df_subset['new'])\n",
    "\n",
    "# set xrange\n",
    "ax.set_xlim(df_subset['date'].min(), df_subset['date'].max())\n",
    "\n",
    "# set title\n",
    "ax.set_title(country + ' (new covid deaths)')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The online repository is updated daily, so we can simply re-run the program everytime we want the newest numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let us improve our program by placing it into two functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `get_deaths` take the name of a country, and extracts the data for that country and wrangles it into a suitable format. It returns a tidy `DataFrame` containing daily time series for total and new covid deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deaths(country):\n",
    "    \n",
    "    # define url\n",
    "    url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "\n",
    "    # import file from url\n",
    "    df_full = pd.read_csv(url)\n",
    "\n",
    "    # drop columns\n",
    "    df_full.drop(['Province/State', 'Lat', 'Long'], axis = 1, inplace = True)\n",
    "\n",
    "    # extract country\n",
    "    df_subset = df_full[df_full['Country/Region'] == country].copy()\n",
    "    \n",
    "    # melt df\n",
    "    df_subset = df_subset.melt(id_vars = ['Country/Region'], var_name = 'date', value_name = 'deaths')\n",
    "\n",
    "    # convert to datetime\n",
    "    df_subset['date'] = pd.to_datetime(df_subset['date'], format = '%m/%d/%y')\n",
    "\n",
    "    # rename\n",
    "    df_subset.columns = ['country', 'date', 'total']\n",
    "    \n",
    "    # take difference\n",
    "    df_subset['new'] = df_subset['total'].diff()\n",
    "\n",
    "    \n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `plot_deaths` takes a `DataFrame` with the country-specific time series and a string indicating whether we want to plot total or new covid deaths. It returns a plot of total or new deaths over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deaths(ylabel, country, df):\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize = (10, 3))\n",
    "\n",
    "    ax.plot(df['date'],\n",
    "            df[ylabel])\n",
    "\n",
    "    # set xrange\n",
    "    ax.set_xlim(df['date'].min(), df['date'].max())\n",
    "\n",
    "    # set title\n",
    "    ax.set_title(country + ' (' + ylabel + ' covid deaths)')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the function to extract and plot covid deaths for any country in the online data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Norway'\n",
    "#country = 'Sweden'\n",
    "#country = 'Denmark'\n",
    "df_subset = get_deaths(country)\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_deaths('new', country, df_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "    <p> Notice that <code>get_plot</code> returns strange plots for some countries, e.g. Denmark. Inspect the output  of <code>get_deaths</code> and the online data set and see if you can figure out what is causing this. Fix <code>get_deaths</code> so that we get correct plots for all countries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary> Click to expand!</summary>\n",
    "<p> \n",
    "\n",
    "```c#\n",
    "\n",
    "# Notice that some countries are split over multiple rows in the online data. That is because they report covid deaths \n",
    "# seperately for the different regions/states in that country. \n",
    "# We can fix our program by simply adding an additional line of code in get_data that sums all of the deaths across the \n",
    "# regions/state for each country.\n",
    "\n",
    "\n",
    "def get_deaths(country):\n",
    "    \n",
    "    # define url\n",
    "    url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "\n",
    "    # import file from url\n",
    "    df_full = pd.read_csv(url)\n",
    "\n",
    "    # drop columns\n",
    "    df_full.drop(['Province/State', 'Lat', 'Long'], axis = 1, inplace = True)\n",
    "    \n",
    "    # sum all provinces/states\n",
    "    df_full = df_full.groupby('Country/Region').sum().reset_index()\n",
    "\n",
    "    # extract country\n",
    "    df_subset = df_full[df_full['Country/Region'] == country].copy()\n",
    "    \n",
    "    # melt df\n",
    "    df_subset = df_subset.melt(id_vars = ['Country/Region'], var_name = 'date', value_name = 'deaths')\n",
    "\n",
    "    # convert to datetime\n",
    "    df_subset['date'] = pd.to_datetime(df_subset['date'], format = '%m/%d/%y')\n",
    "\n",
    "    # rename\n",
    "    df_subset.columns = ['country', 'date', 'total']\n",
    "    \n",
    "    # take difference\n",
    "    df_subset['new'] = df_subset['total'].diff()\n",
    "\n",
    "    \n",
    "    return df_subset\n",
    "\n",
    "\n",
    "def plot_deaths(ylabel, country, df):\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize = (10, 3))\n",
    "\n",
    "    ax.plot(df['date'],\n",
    "            df[ylabel])\n",
    "\n",
    "    # set xrange\n",
    "    ax.set_xlim(df['date'].min(), df['date'].max())\n",
    "\n",
    "    # set title\n",
    "    ax.set_title(country + ' (' + ylabel + ' covid deaths)')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "country = 'Denmark'\n",
    "df_subset = get_deaths(country)\n",
    "plot_deaths('new', country, df_subset)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Yahoo finance**\n",
    "\n",
    "Yahoo finance contains historical data on price and trading volume for many different stocks. Yahoo finance used to have an official API, but it was shutdown in 2017. However, we can download historical data by scraping it directly off the web site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract the historical data for [Apple](https://finance.yahoo.com/quote/AAPL/history?p=AAPL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data directly from the URL (we get the url from right-clicking the \"download\" button and pressing \"save as...\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://query1.finance.yahoo.com/v7/finance/download/AAPL?period1=1603411200&period2=1619136000&interval=1d&events=history&includeAdjustedClose=true'\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = pd.read_csv(url)\n",
    "\n",
    "apple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the URL contains several \"parameters\". Let us store these parameters in variables and then concat the URL back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'AAPL'       # ticker name\n",
    "period1 = 1603411200  # start period\n",
    "period2 = 1619049600  # end period\n",
    "\n",
    "url = 'https://query1.finance.yahoo.com/v7/finance/download/' + ticker + '?period1=' + str(period1) + '&period2=' + str(period2) + '&interval=1d&events=history&includeAdjustedClose=true'\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us download historical data for Apple for every weekday last year. \n",
    "\n",
    "Notice that the time periods are measured in Unix time, i.e. the number of seconds that have elapsed since midnight on January 1, 1970. This point of reference is known as the Unix epoch. It is common for computer systems to use Unix time. \n",
    "\n",
    "We can convert between Unix time and a date by using the `datetime` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`datetime` has a function called `datetime` that we can use to create convert date to timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.datetime(2021, 1, 1, 23, 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply the function `timestamp` to convert the timestamp to the number of second between that date and the Unix Epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime(2021, 1, 1, 23, 59).timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define periods\n",
    "period1 = int(datetime(2021, 1, 1, 23, 59).timestamp())\n",
    "period2 = int(datetime(2021, 12, 31, 23, 59).timestamp())\n",
    "\n",
    "print(period1)\n",
    "print(period2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ticker\n",
    "ticker = 'AAPL'\n",
    "\n",
    "# define url\n",
    "url = 'https://query1.finance.yahoo.com/v7/finance/download/' + ticker + '?period1=' + str(period1) + '&period2=' + str(period2) + '&interval=1d&events=history&includeAdjustedClose=true'\n",
    "\n",
    "# import data\n",
    "apple = pd.read_csv(url)\n",
    "\n",
    "apple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us instead extract historical data for Amazon. The ticker for Amazon is `AMZN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ticker\n",
    "ticker = 'AMZN'\n",
    "\n",
    "# define url\n",
    "url = 'https://query1.finance.yahoo.com/v7/finance/download/' + ticker + '?period1=' + str(period1) + '&period2=' + str(period2) + '&interval=1d&events=history&includeAdjustedClose=true'\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "amazon = pd.read_csv(url)\n",
    "\n",
    "amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "    <p> The file <code>closing_prices.csv</code> contains the daily closing price in 2020 for ten different companies. Import the file and create a list of the tickers. Use this list of tickers to extract the daily opening price for the ten companies from Yahoo finance. Store the daily opening prices in a file called <code>opening_prices.csv</code> on your computer. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary> Click to expand!</summary>\n",
    "<p> \n",
    "\n",
    "```c#\n",
    "\n",
    "# import file\n",
    "df_close = pd.read_csv('data/closing_prices.csv')\n",
    "\n",
    "# extract tickers\n",
    "tickers = df_close['Stock'].unique()\n",
    "\n",
    "# define periods\n",
    "period1 = int(datetime(2020, 1, 1, 23, 59).timestamp())\n",
    "period2 = int(datetime(2020, 12, 31, 23, 59).timestamp())\n",
    "\n",
    "# define folder\n",
    "df_lst = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    \n",
    "    # define url\n",
    "    url = 'https://query1.finance.yahoo.com/v7/finance/download/' + ticker + '?period1=' + str(period1) + '&period2=' + str(period2) + '&interval=1d&events=history&includeAdjustedClose=true'\n",
    "    \n",
    "    # extract data\n",
    "    temp_df = pd.read_csv(url)\n",
    "    \n",
    "    # keep only open price and add stock name\n",
    "    temp_df = temp_df[['Date', 'Open']].copy()\n",
    "    temp_df['Stock'] = ticker\n",
    "\n",
    "    # append to list\n",
    "    df_lst.append(temp_df)\n",
    "\n",
    "# concat to single df (and reset index)\n",
    "df_open = pd.concat(df_lst).reset_index(drop = True)\n",
    "\n",
    "df_open.to_csv('data/opening_prices.csv', index = False)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape HTML tables\n",
    "\n",
    "Many web sites display data in the form of HTML tables.\n",
    "\n",
    "`pandas` has a function, `read_html`, that takes an URL as a parameter, and returns all HTML tables found on that web site as a list of `DataFrame`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Wikipedia\n",
    "\n",
    "[This](https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)) Wikipedia page contains a table with countries and their estimated GDP by IMF, World Bank and United Nations. We want to scrape the information in this table of the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define url\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n",
    "\n",
    "# scrape web page for html tables\n",
    "tables = pd.read_html(url)\n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `read_html` ended up scraping more than the one table that we are after... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the page source of the url (search for \"table class\"), we can see that the table that we are after belongs to the table class `wikitable sortable static-row-numbers plainrowheaders srn-white-background`.\n",
    "\n",
    "We can narrow down the number of tables being scraped by giving the parameter `attrs` a dictionary where we specify that we only want the tables that have `class` equal to a `wikitable sortable static-row-numbers plainrowheaders srn-white-background`.\n",
    "\n",
    "In addition, we set the parameter `header` equal to `0` in order to make sure that the first row in each table is used as the column labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n",
    "\n",
    "tables = pd.read_html(\n",
    "    url, \n",
    "    header = 0, \n",
    "    attrs = {'class' : 'wikitable sortable static-row-numbers plainrowheaders srn-white-background'}\n",
    ")\n",
    "\n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract the GDP data from the United Nations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract table \n",
    "df_gdp = tables[0]\n",
    "\n",
    "# rename and extract columns\n",
    "df_gdp.rename(columns = {'Country/Territory' : 'Country', 'United Nations[12]' : 'GDP'}, inplace = True)\n",
    "df_gdp = df_gdp[['Country', 'GDP']].copy()\n",
    "\n",
    "df_gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first row\n",
    "df_gdp.drop(0, inplace = True)\n",
    "\n",
    "# convert gdp to float\n",
    "df_gdp['GDP'] = df_gdp['GDP'].astype(float)\n",
    "\n",
    "# drop missing\n",
    "df_gdp.dropna(inplace = True)\n",
    "\n",
    "print('Number of countries: ' + str(df_gdp['Country'].nunique()))\n",
    "df_gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gdp['Country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)) Wikipedia page contains information on countries and their estimated population by the United Nations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define url\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)'\n",
    "\n",
    "# scrape html tables of web page\n",
    "tables = pd.read_html(url)\n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract table\n",
    "df_pop = tables[0]\n",
    "\n",
    "# rename and extract columns\n",
    "df_pop.rename(columns = {'Country/Area' : 'Country', 'Population(1 July 2019)' : 'pop'}, inplace = True)\n",
    "df_pop = df_pop[['Country', 'pop']].copy()\n",
    "\n",
    "df_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove parenthesis and square brackets from country names\n",
    "df_pop['Country'] = df_pop['Country'].str.split('(', expand = True)[0]\n",
    "df_pop['Country'] = df_pop['Country'].str.split('[', expand = True)[0]\n",
    "\n",
    "# drop world\n",
    "df_pop = df_pop[df_pop['Country'] != 'World'].copy()\n",
    "\n",
    "print('Number of countries: ' + str(df_pop['Country'].nunique()))\n",
    "df_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pop['Country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the `DataFrame`s in order to estimate countries' GDP per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join\n",
    "df = df_gdp.merge(df_pop, on = 'Country', how = 'inner')\n",
    "\n",
    "# calculate GDP per capita (multiply with 1,000,000 since GDP is measured in million $)\n",
    "df['GDP_pc'] = df['GDP']*1000000 / df['pop']\n",
    "\n",
    "print('Number of countries: ' + str(df['Country'].nunique()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (14, 4))\n",
    "\n",
    "ax[0].scatter(df['GDP_pc'], df['pop'] / 1000000)\n",
    "ax[0].set_ylabel('Population (in millions)')\n",
    "ax[0].set_xlabel('GDP per capita (in dollars)')\n",
    "\n",
    "ax[1].hist(df['GDP_pc'], bins = 30)\n",
    "ax[1].set_ylabel('Number of countries')\n",
    "ax[1].set_xlabel('GDP per capita (in dollars)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "    <p> <a href=\"https://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy\">This</a> Wikipedia site contains information on life expectancy for countries in the world. Notice that the site contains many different tables. Choose the table with the estimated life expectancies from UNDP 2019. \n",
    "        \n",
    "Use this data to create a scatter plot between country GDP per capita and life expectancy, with each marker weighted by country population.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary> Click to expand!</summary>\n",
    "<p> \n",
    "\n",
    "```c#\n",
    "### Tidy data ###\n",
    "\n",
    "# scrape tables\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy'\n",
    "tables = pd.read_html(\n",
    "    url,       \n",
    "    header = 0,\n",
    "    attrs = {'class' : 'wikitable sortable static-row-numbers plainrowheaders srn-white-background'}\n",
    ")\n",
    "print(len(tables))\n",
    "\n",
    "# extract and rename data\n",
    "df_life = tables[1]\n",
    "df_life.rename(columns = {'Countries and regions' : 'Country', 'Life expectancy at birth' : 'life_exp'}, inplace = True)\n",
    "df_life = df_life[['Country', 'life_exp']].copy()\n",
    "\n",
    "# drop first row\n",
    "df_life.drop(0, inplace = True)\n",
    "\n",
    "# convert to float\n",
    "df_life['life_exp'] = df_life['life_exp'].astype(float)\n",
    "\n",
    "print('Number of countries: ' + str(df_life['Country'].nunique()))\n",
    "\n",
    "# merge population and life expectency data (inner join)\n",
    "df2 = df_life.merge(df, on = 'Country', how = 'inner')\n",
    "\n",
    "print('Number of countries: ' + str(df2['Country'].nunique()))\n",
    "\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 4))\n",
    "\n",
    "ax.scatter(\n",
    "    df2['GDP_pc'],             # x-values\n",
    "    df2['life_exp'],           # y-values\n",
    "    s = df2['pop'] / 1000000   # population weights (must divide by 1000000 to make markers visible)\n",
    ")\n",
    "\n",
    "ax.set_ylabel('Life expectancy (in yrs)')\n",
    "ax.set_xlabel('GDP per capita (in dollars)')\n",
    "\n",
    "ax.set_title('Life expectency vs GDP per capita, United Nations (2019)')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "`pandas` is super useful for web scraping! As you have seen, we have scraped many different web sites using just `pandas`.\n",
    "\n",
    "However, there are some limitations to what we can use `pandas` for:\n",
    "- `read_csv` can only be used to extract data that is in the form of csv files.\n",
    "- `read_html` can only be used to extract data that is in the form of HTML tables. \n",
    "\n",
    "If you want to extract data from a web site that is not in the form of a csv file or HTML table, you need to use something more powerful, e.g. `requests` and `BeautifulSoup`. See [this](https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/) or [this](https://betterprogramming.pub/how-to-use-pandas-for-web-scraping-not-enough-try-beautiful-soup-98d0362d5bb1) tutorial for how to use these packages to scrape additional HTML data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
